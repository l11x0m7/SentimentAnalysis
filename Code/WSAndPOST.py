# -*- encoding=utf-8 -*-
import sys 
import os
import jieba
import json
import time
from argparse import ArgumentParser

"""
	åŠŸèƒ½ï¼šå¯¹è¯­æ–™è¿›è¡Œåˆ†è¯å’Œè¯æ€§æ ‡æ³¨
"""

# åˆ†å¥ï¼šä¸€å¥è¯ä¸ä¸€å®šè¦ä»¥å¥å·ç»“å°¾ï¼Œç”±äºä¸å…³è”ä¸Šä¸‹æ–‡ï¼Œæ‰€ä»¥ä½¿ç”¨ä»»æ„æ ‡ç‚¹åˆ†å¥ã€‚
def SentenceSegmentation(paragraph):

	cut_list = list(u',.?!:;ï½ï¼Œã€‚ï¼šï¼›ï¼â€¦ï¼Ÿ~')
	head = 0
	tail = 0
	sentence_list = list()
	for each in paragraph:
		if each in cut_list:
			if head<tail:
				sentence_list.append(paragraph[head:tail])
			head = tail + 1
			tail = head
		else:
			tail += 1
	if head<tail:
		sentence_list.append(paragraph[head:tail])
	return sentence_list

# åˆ†è¯ï¼šæä¾›ç»“å·´åˆ†è¯å’Œltpåˆ†è¯
def WordSegmentation(sentence, type='jieba'):
	if type == 'jieba':
		from jieba import posseg
		jieba.load_userdict('../Dict/NetWords/networds.txt')
		# jieba.load_userdict('../Dict/Emoji/emoji2.txt')
		jieba.load_userdict('../Dict/NetWords/amazon.txt')
		# jieba.add_word('ğŸ˜', 20)
		word_tag_list = posseg.cut(sentence.encode('utf-8'))
		word_list = list()
		for each in word_tag_list:
			each = str(each).split('/')
			word_list.append((each[0], each[1]))
		return word_list
	elif type == 'ltp':
		url_get_base="http://api.ltp-cloud.com/analysis/?"
		api_key="Y5a5D3B4xp9ujH4nyDUXvVlNNOCfyuhftwrXWVbA"
		format='plain'
		pattern='ws'	# wsä¸ºåˆ†è¯
		result=urllib2.urlopen\
		("%sapi_key=%s&text=%s&format=%s&pattern=%s" % \
			(url_get_base, api_key, sentence.encode('utf-8'), format, pattern))
		content=result.read().strip()
		return content
	else:
		return ""

# å¯¹æ•´ä¸ªè¯­æ–™åˆ†è¯ï¼Œå¹¶ä¿å­˜
# params:typeï¼šåˆ†è¯ç±»å‹ï¼ŒåŒ…æ‹¬ltpå’Œjiebaåˆ†è¯ï¼›
# startï¼šèµ·å§‹è¡Œï¼›
# AddorWriteï¼šé‡å†™æ–‡ä»¶æˆ–æœ«å°¾è¿½åŠ 
def WSofFile(filepath, savepath, type='jieba', start=1, AddorWrite='w'):
	with open(savepath, AddorWrite) as fw:
		with open(filepath) as fr:
			cur = start
			files = fr.readlines()
			while cur <= len(files):
				paragraph = list()
				item = files[cur-1].strip().split('\t')
				sentence_list = SentenceSegmentation(item[0].decode())
				i = 0
				while i < len(sentence_list):
					try:
						word_list = WordSegmentation(sentence_list[i].decode(), type)
					except urllib2.HTTPError as e:
						print e
						print 'Retrying No.%d...' % cur
						continue
					i += 1
					paragraph.append(word_list)
					time.sleep(0.1)
				cur += 1
				print json.dumps(paragraph, ensure_ascii=False)
				fw.write(json.dumps(paragraph, ensure_ascii=False) + '\t' + item[1] + '\n')


if __name__ == '__main__':
	reload(sys)
	sys.setdefaultencoding('utf-8')

	parser = ArgumentParser(description='Word Segment and POS Tagging')
	parser.add_argument('--filepath', dest='filepath')
	parser.add_argument('--savepath', dest='savepath')
	parser.add_argument('--wstype', dest='wstype')
	parser.add_argument('--startline', dest='startline')
	parser.add_argument('--addtype', dest='addtype')
	args = parser.parse_args()
	WSofFile(args.filepath, args.savepath, args.wstype, int(args.startline), args.addtype)


